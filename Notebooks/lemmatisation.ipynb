{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "* Check your Python version and install the **CLTK** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a Single Chapter\n",
    "1. **Familiarize yourself with the document** you want to lemmatize.\n",
    "2. **Read the document carefully.**\n",
    "3. **Document analysis:**\n",
    "\n",
    "   * List the first 80 characters.\n",
    "   * Count the total number of characters (letters, spaces, punctuation).\n",
    "   * Tokenize the document to count the number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"texts/C_II_all.txt\") as f:\n",
    "    Chapter2_full = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet = Chapter2_full[:80]\n",
    "chars   = len(Chapter2_full)\n",
    "tokens  = len(Chapter2_full.split())\n",
    "\n",
    "print(\"80 first worlds:\", snippet) \n",
    "print(\"Character count:\", chars) \n",
    "print(\"Approximate token count:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Lemmatization Pipeline\n",
    "\n",
    "1. Import the NLP module.\n",
    "2. Apply lemmatization to the document.\n",
    "3. Print the first 20 lemmatized words as a quick test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk import NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cltk_nlp = NLP(language=\"lat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time  cltk_doc = cltk_nlp.analyze(text=Chapter2_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lemmas\n",
    "print(cltk_doc.lemmata[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting the Document for Data Visualization\n",
    "\n",
    "* Create a `.txt` file with **one word per line** (for archival purposes).\n",
    "* Create a `.txt` file containing the full chapter with the **entirely lemmatized text** for visualization (e.g., co-occurrence, topic modeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming cltk_doc.lemmata contains the lemmata data\n",
    "lemmata = cltk_doc.lemmata\n",
    "\n",
    "# Specify the file path where you want to save the result\n",
    "file_path = \"C_II_lem_col.txt\"\n",
    "\n",
    "# Open the file in write mode ('w'), this will overwrite any existing file with the same name\n",
    "with open(file_path, 'w') as file:\n",
    "    # Write each lemma to the file\n",
    "    for lemma in lemmata:\n",
    "        file.write(f\"{lemma}\\n\")  # Each lemma on a new line\n",
    "\n",
    "print(f\"Results saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstituer le texte pour l'analyse de coocurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming cltk_doc.lemmata contains the lemmata data\n",
    "lemmata = cltk_doc.lemmata\n",
    "\n",
    "# Specify the file path where you want to save the result\n",
    "file_path = \"C_II_lem.txt\"\n",
    "\n",
    "# Open the file in write mode ('w'), this will overwrite any existing file with the same name\n",
    "with open(file_path, 'w') as file:\n",
    "    # Join the lemmata with spaces between them and write to the file\n",
    "    file.write(\" \".join(lemmata))  # All words in one line, separated by spaces\n",
    "\n",
    "print(f\"Results saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a Batch of Documents\n",
    "\n",
    "1. Work with a large dataset using the `glob` function.\n",
    "\n",
    "   * Example path: `\"texts/*/*.txt\"`\n",
    "2. Create the necessary folders and subfolders.\n",
    "3. Run the NLP pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Äéê§Ä CLTK version '1.4.0'. When using the CLTK in research, please cite: https://aclanthology.org/2021.acl-demo.3/\n",
      "\n",
      "Pipeline for language 'Latin' (ISO: 'lat'): `LatinNormalizeProcess`, `LatinStanzaProcess`, `LatinEmbeddingsProcess`, `StopsProcess`, `LatinLexiconProcess`.\n",
      "\n",
      "‚∏ñ ``LatinStanzaProcess`` using Stanza model from the Stanford NLP Group: https://stanfordnlp.github.io/stanza/ . Please cite: https://arxiv.org/abs/2003.07082\n",
      "‚∏ñ ``LatinEmbeddingsProcess`` using word2vec model by University of Oslo from http://vectors.nlpl.eu/ . Please cite: https://aclanthology.org/W17-0237/\n",
      "‚∏ñ ``LatinLexiconProcess`` using Lewis's *An Elementary Latin Dictionary* (1890).\n",
      "\n",
      "‚∏é To suppress these messages, instantiate ``NLP()`` with ``suppress_banner=True``.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from cltk import NLP\n",
    "\n",
    "# Initialize CLTK only once\n",
    "cltk_nlp = NLP(language=\"lat\")\n",
    "\n",
    "# Input folder pattern: all .txt files inside subfolders e.g. : \"texts/*/*.txt\" ou un dossier pr√©cis  \"texts/Hyperius/*.txt\"\n",
    "INPUT_PATTERN =  \"texts/Unbekannt/*.txt\"\n",
    "\n",
    "# Output folder\n",
    "OUTPUT_DIR = \"lemmatized_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Documents to be Lemmatized\n",
    "\n",
    "* Display the list of documents to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 text files:\n",
      " - texts/Unbekannt/C_II_v2_cl.txt\n",
      " - texts/Unbekannt/C_II_v8_cl.txt\n",
      " - texts/Unbekannt/C_II_v6-7_cl.txt\n",
      " - texts/Unbekannt/C_II_v1_cl.txt\n",
      " - texts/Unbekannt/C_II_v13-14_cl.txt\n",
      " - texts/Unbekannt/C_II_cl.txt\n",
      " - texts/Unbekannt/C_II_v5-6_cl.txt\n",
      " - texts/Unbekannt/C_II_v11-12_cl.txt\n",
      " - texts/Unbekannt/C_II_v15_cl.txt\n",
      " - texts/Unbekannt/C_II_v9_cl.txt\n",
      " - texts/Unbekannt/C_II_v10_cl.txt\n",
      " - texts/Unbekannt/C_II_v3-4_cl.txt\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(INPUT_PATTERN)\n",
    "\n",
    "print(f\"Found {len(files)} text files:\")\n",
    "for f in files:\n",
    "    print(\" -\", f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing a Dataset\n",
    "\n",
    "1. Create a loop to process multiple documents.\n",
    "2. Choose a consistent naming convention for output files:\n",
    "\n",
    "   ```python\n",
    "   filename = \"FolderName_FileName.txt\"\n",
    "   ```\n",
    "\n",
    "3. Run the NLP processing line for each document.\n",
    "4. Save the results in a designated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: texts/Unbekannt/C_II_v2_cl.txt\n",
      "Saved ‚Üí lemmatized_outputs/Unbekannt_C_II_v2_cl_lem.txt\n",
      "\n",
      "Processing: texts/Unbekannt/C_II_v8_cl.txt\n",
      "Saved ‚Üí lemmatized_outputs/Unbekannt_C_II_v8_cl_lem.txt\n",
      "\n",
      "Processing: texts/Unbekannt/C_II_v6-7_cl.txt\n",
      "Saved ‚Üí lemmatized_outputs/Unbekannt_C_II_v6_7_cl_lem.txt\n",
      "\n",
      "Processing: texts/Unbekannt/C_II_v1_cl.txt\n",
      "Saved ‚Üí lemmatized_outputs/Unbekannt_C_II_v1_cl_lem.txt\n",
      "\n",
      "Processing: texts/Unbekannt/C_II_v13-14_cl.txt\n",
      "Saved ‚Üí lemmatized_outputs/Unbekannt_C_II_v13_14_cl_lem.txt\n",
      "\n",
      "Processing: texts/Unbekannt/C_II_cl.txt\n",
      "Saved ‚Üí lemmatized_outputs/Unbekannt_C_II_cl_lem.txt\n",
      "\n",
      "Processing: texts/Unbekannt/C_II_v5-6_cl.txt\n",
      "Saved ‚Üí lemmatized_outputs/Unbekannt_C_II_v5_6_cl_lem.txt\n",
      "\n",
      "Processing: texts/Unbekannt/C_II_v11-12_cl.txt\n",
      "Saved ‚Üí lemmatized_outputs/Unbekannt_C_II_v11_12_cl_lem.txt\n",
      "\n",
      "Processing: texts/Unbekannt/C_II_v15_cl.txt\n",
      "Saved ‚Üí lemmatized_outputs/Unbekannt_C_II_v15_cl_lem.txt\n",
      "\n",
      "Processing: texts/Unbekannt/C_II_v9_cl.txt\n",
      "Saved ‚Üí lemmatized_outputs/Unbekannt_C_II_v9_cl_lem.txt\n",
      "\n",
      "Processing: texts/Unbekannt/C_II_v10_cl.txt\n",
      "Saved ‚Üí lemmatized_outputs/Unbekannt_C_II_v10_cl_lem.txt\n",
      "\n",
      "Processing: texts/Unbekannt/C_II_v3-4_cl.txt\n",
      "Saved ‚Üí lemmatized_outputs/Unbekannt_C_II_v3_4_cl_lem.txt\n",
      "\n",
      "All files processed!\n"
     ]
    }
   ],
   "source": [
    "#open the loop\n",
    "\n",
    "for file_path in files:\n",
    "    print(\"\\nProcessing:\", file_path)\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Extract folder name + filename\n",
    "    # -------------------------------------------------------------------\n",
    "    folder = os.path.basename(os.path.dirname(file_path))     # e.g. \"Bullinger\"\n",
    "    filename = os.path.basename(file_path)                    # e.g. \"C_II_v5-7_cl.txt\"\n",
    "\n",
    "    # Clean base name: remove extension + replace hyphens\n",
    "    base = os.path.splitext(filename)[0].replace(\"-\", \"_\")\n",
    "\n",
    "    # Output name format: Folder_Filename_lem.txt\n",
    "    output_name = f\"{folder}_{base}_lem.txt\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Read the file\n",
    "    # -------------------------------------------------------------------\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # CLTK NLP\n",
    "    # -------------------------------------------------------------------\n",
    "    cltk_doc = cltk_nlp.analyze(text=text)\n",
    "    lemmas = [w.lemma for w in cltk_doc.words]\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Save output\n",
    "    # -------------------------------------------------------------------\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        out.write(\"\\n\".join(lemmas))\n",
    "\n",
    "    print(\"Saved ‚Üí\", output_path)\n",
    "\n",
    "print(\"\\nAll files processed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Processed Documents\n",
    "\n",
    "* Keep track of the documents that have been successfully lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated files:\n",
      " - Aretius_C_II_cl_lem.txt\n",
      " - Aretius_C_II_v10_cl_lem.txt\n",
      " - Aretius_C_II_v11_cl_lem.txt\n",
      " - Aretius_C_II_v12_cl_lem.txt\n",
      " - Aretius_C_II_v13_cl_lem.txt\n",
      " - Aretius_C_II_v14_cl_lem.txt\n",
      " - Aretius_C_II_v15_cl_lem.txt\n",
      " - Aretius_C_II_v1_cl_lem.txt\n",
      " - Aretius_C_II_v1b_cl_lem.txt\n",
      " - Aretius_C_II_v2_cl_lem.txt\n",
      " - Aretius_C_II_v2b_cl_lem.txt\n",
      " - Aretius_C_II_v3_cl_lem.txt\n",
      " - Aretius_C_II_v6_cl_lem.txt\n",
      " - Aretius_C_II_v6b_cl_lem.txt\n",
      " - Aretius_C_II_v7_cl_lem.txt\n",
      " - Aretius_C_II_v8_cl_lem.txt\n",
      " - Aretius_C_II_v9_cl_lem.txt\n",
      " - Bugenhagen_C_II_cl_lem.txt\n",
      " - Bugenhagen_C_II_v11_cl_lem.txt\n",
      " - Bugenhagen_C_II_v1_cl_lem.txt\n",
      " - Bugenhagen_C_II_v4_cl_lem.txt\n",
      " - Bugenhagen_C_II_v5_cl_lem.txt\n",
      " - Bugenhagen_C_II_v6_cl_lem.txt\n",
      " - Bugenhagen_C_II_v8_cl_lem.txt\n",
      " - Bugenhagen_C_II_v8b_cl_lem.txt\n",
      " - Bullinger_C_II_cl_lem.txt\n",
      " - Bullinger_C_II_v11_15_cl_lem.txt\n",
      " - Bullinger_C_II_v15ep_cl_lem.txt\n",
      " - Bullinger_C_II_v15epb_cl_lem.txt\n",
      " - Bullinger_C_II_v1_2_cl_lem.txt\n",
      " - Bullinger_C_II_v1_cl_lem.txt\n",
      " - Bullinger_C_II_v3_4_cl_lem.txt\n",
      " - Bullinger_C_II_v5_7_cl_lem.txt\n",
      " - Bullinger_C_II_v8_cl_lem.txt\n",
      " - Bullinger_C_II_v9_10_cl_lem.txt\n",
      " - Cajetan_C_II_cl_lem.txt\n",
      " - Calvin_C_II_v11_15_cl_lem.txt\n",
      " - Calvin_C_II_v1_2_cl_lem.txt\n",
      " - Calvin_C_II_v2_4_cl_lem.txt\n",
      " - Calvin_C_II_v5_7_cl_lem.txt\n",
      " - Calvin_C_II_v8_10_cl_lem.txt\n",
      " - Hyperius_C_II_cl_lem.txt\n",
      " - Hyperius_C_II_v11_12_cl_lem.txt\n",
      " - Hyperius_C_II_v13_14_cl_lem.txt\n",
      " - Hyperius_C_II_v15_cl_lem.txt\n",
      " - Hyperius_C_II_v15ep_cl_lem.txt\n",
      " - Hyperius_C_II_v1_2_cl_lem.txt\n",
      " - Hyperius_C_II_v3_4_cl_lem.txt\n",
      " - Hyperius_C_II_v5_6_cl_lem.txt\n",
      " - Hyperius_C_II_v7_cl_lem.txt\n",
      " - Hyperius_C_II_v8_10_cl_lem.txt\n",
      " - Hyperius_C_II_v8_cl_lem.txt\n",
      " - Lambertus_C_II_cl_lem.txt\n",
      " - Lambertus_C_II_v10_cl_lem.txt\n",
      " - Lambertus_C_II_v11_cl_lem.txt\n",
      " - Lambertus_C_II_v12_cl_lem.txt\n",
      " - Lambertus_C_II_v12b_cl_lem.txt\n",
      " - Lambertus_C_II_v13_cl_lem.txt\n",
      " - Lambertus_C_II_v14_cl_lem.txt\n",
      " - Lambertus_C_II_v15_cl_lem.txt\n",
      " - Lambertus_C_II_v1_cl_lem.txt\n",
      " - Lambertus_C_II_v2_cl_lem.txt\n",
      " - Lambertus_C_II_v3_cl_lem.txt\n",
      " - Lambertus_C_II_v4_cl_lem.txt\n",
      " - Lambertus_C_II_v5_cl_lem.txt\n",
      " - Lambertus_C_II_v6_cl_lem.txt\n",
      " - Lambertus_C_II_v7_cl_lem.txt\n",
      " - Lambertus_C_II_v8_cl_lem.txt\n",
      " - Lambertus_C_II_v9_cl_lem.txt\n",
      " - Lefevre_C_II_cl_lem.txt\n",
      " - Pellicanus_C_II_v11_12_cl_lem.txt\n",
      " - Pellicanus_C_II_v13_14_cl_lem.txt\n",
      " - Pellicanus_C_II_v15_cl_lem.txt\n",
      " - Pellicanus_C_II_v1_2_cl_lem.txt\n",
      " - Pellicanus_C_II_v2_cl_lem.txt\n",
      " - Pellicanus_C_II_v3_4_cl_lem.txt\n",
      " - Pellicanus_C_II_v5_6_cl_lem.txt\n",
      " - Pellicanus_C_II_v6_7_cl_lem.txt\n",
      " - Pellicanus_C_II_v8_cl_lem.txt\n",
      " - Pellicanus_C_II_v9_10_cl_lem.txt\n",
      " - Unbekannt_C_II_cl_lem.txt\n",
      " - Unbekannt_C_II_v10_cl_lem.txt\n",
      " - Unbekannt_C_II_v11_12_cl_lem.txt\n",
      " - Unbekannt_C_II_v13_14_cl_lem.txt\n",
      " - Unbekannt_C_II_v15_cl_lem.txt\n",
      " - Unbekannt_C_II_v1_cl_lem.txt\n",
      " - Unbekannt_C_II_v2_cl_lem.txt\n",
      " - Unbekannt_C_II_v3_4_cl_lem.txt\n",
      " - Unbekannt_C_II_v5_6_cl_lem.txt\n",
      " - Unbekannt_C_II_v6_7_cl_lem.txt\n",
      " - Unbekannt_C_II_v8_cl_lem.txt\n",
      " - Unbekannt_C_II_v9_cl_lem.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated files:\")\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    print(\" -\", f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Documents for Data Visualization\n",
    "\n",
    "* Create a `.txt` file with **one word per line** (for archives).\n",
    "* Create a `.txt` file containing each verses and his commentary with the **fully lemmatized text** (for co-occurrence and topic modeling visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted ‚Üí lemmatized/Aretius_C_II_v14_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v4_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bullinger_C_II_v5_7_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Hyperius_C_II_v3_4_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Calvin_C_II_v5_7_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Unbekannt_C_II_v10_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Hyperius_C_II_v8_10_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bugenhagen_C_II_v5_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Pellicanus_C_II_v3_4_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Calvin_C_II_v2_4_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v2b_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bullinger_C_II_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v14_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bugenhagen_C_II_v4_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v8_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v1_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lefevre_C_II_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v13_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Calvin_C_II_v11_15_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v5_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v10_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bullinger_C_II_v15epb_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Pellicanus_C_II_v2_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v6b_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Pellicanus_C_II_v5_6_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Hyperius_C_II_v8_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v7_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v3_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Hyperius_C_II_v1_2_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Unbekannt_C_II_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bullinger_C_II_v1_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Unbekannt_C_II_v2_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bullinger_C_II_v1_2_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Calvin_C_II_v8_10_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Unbekannt_C_II_v13_14_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Hyperius_C_II_v5_6_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v1b_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Pellicanus_C_II_v1_2_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bullinger_C_II_v3_4_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Hyperius_C_II_v13_14_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Pellicanus_C_II_v6_7_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v6_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Unbekannt_C_II_v15_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Hyperius_C_II_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Cajetan_C_II_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v1_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v2_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Pellicanus_C_II_v9_10_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v9_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v10_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Unbekannt_C_II_v3_4_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bugenhagen_C_II_v8_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bullinger_C_II_v8_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bugenhagen_C_II_v8b_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Pellicanus_C_II_v15_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Pellicanus_C_II_v8_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bugenhagen_C_II_v11_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v11_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v3_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v6_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v15_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v11_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v7_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v8_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Unbekannt_C_II_v5_6_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bullinger_C_II_v9_10_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bullinger_C_II_v15ep_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v12_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Calvin_C_II_v1_2_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bugenhagen_C_II_v6_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v9_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bugenhagen_C_II_v1_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v12_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v2_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Hyperius_C_II_v15_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Hyperius_C_II_v7_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bugenhagen_C_II_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Hyperius_C_II_v15ep_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Aretius_C_II_v13_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Hyperius_C_II_v11_12_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v15_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Lambertus_C_II_v12b_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Unbekannt_C_II_v9_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Unbekannt_C_II_v1_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Unbekannt_C_II_v6_7_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Unbekannt_C_II_v8_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Bullinger_C_II_v11_15_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Unbekannt_C_II_v11_12_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Pellicanus_C_II_v11_12_cl_lem.txt\n",
      "Converted ‚Üí lemmatized/Pellicanus_C_II_v13_14_cl_lem.txt\n",
      "\n",
      "All lemma converted to continuous text!\n"
     ]
    }
   ],
   "source": [
    "# Directory with lemma files\n",
    "LEMMA_DIR = OUTPUT_DIR\n",
    "\n",
    "# Directory for continuous text output\n",
    "CONTINUOUS_DIR = \"lemmatized\"\n",
    "os.makedirs(CONTINUOUS_DIR, exist_ok=True)\n",
    "\n",
    "# Loop over all lemma files\n",
    "for filename in os.listdir(LEMMA_DIR):\n",
    "    if filename.endswith(\"_lem.txt\"):\n",
    "        input_path = os.path.join(LEMMA_DIR, filename)\n",
    "        output_path = os.path.join(CONTINUOUS_DIR, filename)\n",
    "\n",
    "        # Read lemma file\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lemmas = f.read().splitlines()\n",
    "\n",
    "        # Join lemmas into continuous text\n",
    "        continuous_text = \" \".join(lemmas)\n",
    "\n",
    "        # Save\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "            out.write(continuous_text)\n",
    "\n",
    "        print(\"Converted ‚Üí\", output_path)\n",
    "\n",
    "print(\"\\nAll lemma converted to continuous text!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reconstitute the chapter by combining all `.txt` files that share the same text, in alphanumerical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Aretius_all_C_II_cl_lem.txt from 17 files...\n",
      "Creating Lambertus_all_C_II_cl_lem.txt from 17 files...\n",
      "Creating Bullinger_all_C_II_cl_lem.txt from 10 files...\n",
      "Creating Hyperius_all_C_II_cl_lem.txt from 11 files...\n",
      "Creating Calvin_all_C_II_cl_lem.txt from 5 files...\n",
      "Creating Unbekannt_all_C_II_cl_lem.txt from 12 files...\n",
      "Creating Bugenhagen_all_C_II_cl_lem.txt from 8 files...\n",
      "Creating Pellicanus_all_C_II_cl_lem.txt from 10 files...\n",
      "Creating Lefevre_all_C_II_cl_lem.txt from 1 files...\n",
      "Creating Cajetan_all_C_II_cl_lem.txt from 1 files...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from natsort import natsorted   # pip install natsort\n",
    "\n",
    "# Folder containing your .txt files\n",
    "folder = \"lemmatized\"\n",
    "\n",
    "# Get all .txt files in folder\n",
    "files = [f for f in os.listdir(folder) if f.endswith(\".txt\")]\n",
    "\n",
    "# Extract prefix before first underscore\n",
    "# Example: Aretius_C_II_v1 ‚Üí prefix = \"Aretius\"\n",
    "prefixes = {}\n",
    "for f in files:\n",
    "    prefix = f.split(\"_\")[0]\n",
    "    prefixes.setdefault(prefix, []).append(f)\n",
    "\n",
    "# Process each prefix group\n",
    "for prefix, grouped_files in prefixes.items():\n",
    "    # Sort alphanumerically\n",
    "    sorted_files = natsorted(grouped_files)\n",
    "\n",
    "    # Output filename\n",
    "    outfile = f\"{prefix}_all_C_II_cl_lem.txt\"\n",
    "\n",
    "    print(f\"Creating {outfile} from {len(sorted_files)} files...\")\n",
    "\n",
    "    with open(outfile, \"w\", encoding=\"utf-8\") as out:\n",
    "        for fname in sorted_files:\n",
    "            path = os.path.join(folder, fname)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as infile:\n",
    "                out.write(infile.read())\n",
    "                out.write(\"\\n\")   # optional separator\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "* Full documentation is available at [CLTK Docs](https://docs.cltk.org).\n",
    "\n",
    "## Citation\n",
    "\n",
    "When using the CLTK, please cite the following publication:\n",
    "\n",
    "Johnson, Kyle P., Patrick J. Burns, John Stewart, Todd Cook, Cl√©ment Besnier, and William J. B. Mattingly. \"The Classical Language Toolkit: An NLP Framework for Pre-Modern Languages.\" In *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations*, pp. 20-29. 2021. DOI: [10.18653/v1/2021.acl-demo.3](https://doi.org/10.18653/v1/2021.acl-demo.3)\n",
    "\n",
    "**BibTeX entry:**\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{johnson-etal-2021-classical,\n",
    "    title = \"The {C}lassical {L}anguage {T}oolkit: {A}n {NLP} Framework for Pre-Modern Languages\",\n",
    "    author = \"Johnson, Kyle P.  and\n",
    "      Burns, Patrick J.  and\n",
    "      Stewart, John  and\n",
    "      Cook, Todd  and\n",
    "      Besnier, Cl{\\'e}ment  and\n",
    "      Mattingly, William J. B.\",\n",
    "    booktitle = \"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations\",\n",
    "    month = aug,\n",
    "    year = \"2021\",\n",
    "    address = \"Online\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://aclanthology.org/2021.acl-demo.3\",\n",
    "    doi = \"10.18653/v1/2021.acl-demo.3\",\n",
    "    pages = \"20--29\",\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lem3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
